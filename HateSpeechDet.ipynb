{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>As a woman you shouldn't complain about clean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>boy dats cold...tyga dwn bad for cuffin dat h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>You ever fuck a bitch and she start to cry? Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@viva_based she look like a tranny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The shit you hear about me might be true or i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0   As a woman you shouldn't complain about clean...  \n",
       "1   boy dats cold...tyga dwn bad for cuffin dat h...  \n",
       "2   You ever fuck a bitch and she start to cry? Y...  \n",
       "3                 @viva_based she look like a tranny  \n",
       "4   The shit you hear about me might be true or i...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count: number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when \n",
    "# hate_speech: number of CF users who judged the tweet to be hate speech\n",
    "# offensive_language: number of CF users who judged the tweet to be offensive\n",
    "# neither: number of CF users who judged the tweet to be neither offensive nor non-offensive\n",
    "# class: class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n",
    "# tweet: text tweet\n",
    "\n",
    "def parse_tweets_until_colon(data):\n",
    "    parsed_tweets = []\n",
    "    for tweet in data['tweet']:\n",
    "        colon_index = tweet.find(':')\n",
    "        if colon_index != -1:\n",
    "            parsed_tweets.append(tweet[colon_index + 1:])\n",
    "        else:\n",
    "            parsed_tweets.append(tweet)\n",
    "    return parsed_tweets\n",
    "\n",
    "data = pd.read_csv('labeled_data.csv')\n",
    "data = data.drop(columns='Unnamed: 0')\n",
    "data['tweet'] = parse_tweets_until_colon(data)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eddy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Eddy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Eddy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>woman shouldnt complain clean hous amp man alw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>boy dat coldtyga dwn bad cuffin dat hoe st place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ever fuck bitch start cri confus shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>viva_bas look like tranni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>shit hear might true might faker bitch told ya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  woman shouldnt complain clean hous amp man alw...  \n",
       "1   boy dat coldtyga dwn bad cuffin dat hoe st place  \n",
       "2              ever fuck bitch start cri confus shit  \n",
       "3                          viva_bas look like tranni  \n",
       "4     shit hear might true might faker bitch told ya  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Cleaning:\n",
    "# Lowercasing: Convert all text to lowercase to ensure uniformity.\n",
    "# Removing Punctuation: Eliminate punctuation marks as they often don't carry much meaning in NLP tasks.\n",
    "# Removing Special Characters: Remove special characters, emojis, URLs, etc., which may not contribute to the task at hand.\n",
    "# Removing Stopwords: Stopwords are common words (e.g., \"the\", \"is\", \"and\") that occur frequently but often carry little information. Removing them can reduce noise in the data.\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def clean_text(data):\n",
    "    cleaned_tweets = []\n",
    "    for tweet in data[\"tweet\"]:\n",
    "        tweet = tweet.lower()  # Convert text to lowercase\n",
    "        tweet = re.sub(r\"[^\\w\\s]\", \"\", tweet)  # Remove punctuation\n",
    "        tweet = re.sub(r\"\\d+\", \"\", tweet)  # Remove numbers\n",
    "        tweet = re.sub(r\"\\s+\", \" \", tweet).strip()  # Remove extra whitespaces\n",
    "        cleaned_tweets.append(tweet)\n",
    "    return cleaned_tweets\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    nostopwords_tweets = []\n",
    "    for tweet in data[\"tweet\"]:\n",
    "        tokens = word_tokenize(tweet)  # Tokenize text\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "        filtered_text = \" \".join(filtered_tokens)  # Join tokens back into a string\n",
    "        nostopwords_tweets.append(filtered_text)\n",
    "    return nostopwords_tweets\n",
    "\n",
    "def apply_stemming(data):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tweets = []\n",
    "    for tweet in data[\"tweet\"]:\n",
    "        tokens = word_tokenize(tweet)  # Tokenize text\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]  # Apply stemming\n",
    "        stemmed_text = \" \".join(stemmed_tokens)  # Join tokens back into a string\n",
    "        stemmed_tweets.append(stemmed_text)\n",
    "    return stemmed_tweets\n",
    "\n",
    "def apply_lemmatization(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tweets = []\n",
    "    for tweet in data[\"tweet\"]:\n",
    "        tokens = word_tokenize(tweet)  # Tokenize text\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Apply lemmatization\n",
    "        lemmatized_text = \" \".join(lemmatized_tokens)  # Join tokens back into a string\n",
    "        lemmatized_tweets.append(lemmatized_text)\n",
    "    return lemmatized_tweets\n",
    "\n",
    "data[\"tweet\"] = clean_text(data)\n",
    "data[\"tweet\"] = remove_stopwords(data)\n",
    "data[\"tweet\"] = apply_stemming(data)\n",
    "data[\"tweet\"] = apply_lemmatization(data)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"class\"], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for training\n",
      "Epoch 1: Average Loss: 0.4117244767881329\n",
      "Epoch 2: Average Loss: 0.28059977674255926\n",
      "Epoch 3: Average Loss: 0.20514537520639056\n",
      "Epoch 4: Average Loss: 0.14681136520924948\n",
      "Epoch 5: Average Loss: 0.09591399780505115\n",
      "Accuracy: 0.8694775065563849\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for training\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to eos_token\n",
    "\n",
    "max_length = 128\n",
    "tokenized_texts_train = tokenizer(list(X_train), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "tokenized_texts_test = tokenizer(list(X_test), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "labels_train = torch.tensor(list(y_train)).to(device)\n",
    "labels_test = torch.tensor(list(y_test)).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(tokenized_texts_train.input_ids, tokenized_texts_train.attention_mask, labels_train)\n",
    "test_dataset = TensorDataset(tokenized_texts_test.input_ids, tokenized_texts_test.attention_mask, labels_test)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_config = GPT2Config.from_pretrained('gpt2', num_labels=3, pad_token_id=tokenizer.pad_token_id)\n",
    "model = GPT2ForSequenceClassification(model_config).to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, no_deprecation_warning=True)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}: Average Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "model.eval()\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predicted_labels.extend(torch.argmax(logits, axis=1).tolist())\n",
    "        true_labels.extend(labels.tolist())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
