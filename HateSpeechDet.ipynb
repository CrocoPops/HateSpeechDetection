{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2ForSequenceClassification, BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "- <strong>count</strong>: number of CrowdFlower users who coded each tweet\n",
    "- <strong>hate_speech</strong>: number of CF users who judged the tweet to be hate speech\n",
    "- <strong>offensive_language</strong>: number of CF users who judged the tweet to be offensive\n",
    "- <strong>neither</strong>: number of CF users who judged the tweet to be neither offensive nor non-offensive\n",
    "- <strong>class</strong>: class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n",
    "- <strong>tweet</strong>: text tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>As a woman you shouldn't complain about clean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>boy dats cold...tyga dwn bad for cuffin dat h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>You ever fuck a bitch and she start to cry? Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@viva_based she look like a tranny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The shit you hear about me might be true or i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0   As a woman you shouldn't complain about clean...  \n",
       "1   boy dats cold...tyga dwn bad for cuffin dat h...  \n",
       "2   You ever fuck a bitch and she start to cry? Y...  \n",
       "3                 @viva_based she look like a tranny  \n",
       "4   The shit you hear about me might be true or i...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensorboard initialization\n",
    "writer = SummaryWriter(log_dir=\"./logs\")\n",
    "\n",
    "def parse_tweets_until_colon(data):\n",
    "    parsed_tweets = []\n",
    "    for tweet in data['tweet']:\n",
    "        colon_index = tweet.find(':')\n",
    "        if colon_index != -1:\n",
    "            parsed_tweets.append(tweet[colon_index + 1:])\n",
    "        else:\n",
    "            parsed_tweets.append(tweet)\n",
    "    return parsed_tweets\n",
    "\n",
    "data = pd.read_csv('labeled_data.csv')\n",
    "data = data.drop(columns='Unnamed: 0')\n",
    "data['tweet'] = parse_tweets_until_colon(data)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning\n",
    "\n",
    "- <strong>Lowercasing</strong>: Convert all text to lowercase to ensure uniformity.\n",
    "- <strong>Removing Punctuation</strong>: Eliminate punctuation marks as they often don't carry much meaning in NLP tasks.\n",
    "- <strong>Removing Special Characters</strong>: Remove special characters, emojis, URLs, etc., which may not contribute to the task at hand.\n",
    "- <strong>Removing Stopwords</strong>: Stopwords are common words (e.g., \"the\", \"is\", \"and\") that occur frequently but often carry little information. Removing them can reduce noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/manuz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/manuz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/manuz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>woman shouldnt complain clean hous amp man alw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>boy dat coldtyga dwn bad cuffin dat hoe st place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ever fuck bitch start cri confus shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>viva_bas look like tranni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>shit hear might true might faker bitch told ya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  woman shouldnt complain clean hous amp man alw...  \n",
       "1   boy dat coldtyga dwn bad cuffin dat hoe st place  \n",
       "2              ever fuck bitch start cri confus shit  \n",
       "3                          viva_bas look like tranni  \n",
       "4     shit hear might true might faker bitch told ya  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def clean_text(data):\n",
    "    cleaned_tweets = []\n",
    "    for tweet in data:\n",
    "        tweet = tweet.lower()  # Convert text to lowercase\n",
    "        tweet = re.sub(r\"[^\\w\\s]\", \"\", tweet)  # Remove punctuation\n",
    "        tweet = re.sub(r\"\\d+\", \"\", tweet)  # Remove numbers\n",
    "        tweet = re.sub(r\"\\s+\", \" \", tweet).strip()  # Remove extra whitespaces\n",
    "        cleaned_tweets.append(tweet)\n",
    "    return cleaned_tweets\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    nostopwords_tweets = []\n",
    "    for tweet in data:\n",
    "        tokens = word_tokenize(tweet)  # Tokenize text\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "        filtered_text = \" \".join(filtered_tokens)  # Join tokens back into a string\n",
    "        nostopwords_tweets.append(filtered_text)\n",
    "    return nostopwords_tweets\n",
    "\n",
    "def apply_stemming(data):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tweets = []\n",
    "    for tweet in data:\n",
    "        tokens = word_tokenize(tweet)  # Tokenize text\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]  # Apply stemming\n",
    "        stemmed_text = \" \".join(stemmed_tokens)  # Join tokens back into a string\n",
    "        stemmed_tweets.append(stemmed_text)\n",
    "    return stemmed_tweets\n",
    "\n",
    "def apply_lemmatization(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tweets = []\n",
    "    for tweet in data:\n",
    "        tokens = word_tokenize(tweet)  # Tokenize text\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Apply lemmatization\n",
    "        lemmatized_text = \" \".join(lemmatized_tokens)  # Join tokens back into a string\n",
    "        lemmatized_tweets.append(lemmatized_text)\n",
    "    return lemmatized_tweets\n",
    "\n",
    "data[\"tweet\"] = clean_text(data[\"tweet\"])\n",
    "data[\"tweet\"] = remove_stopwords(data[\"tweet\"])\n",
    "data[\"tweet\"] = apply_stemming(data[\"tweet\"])\n",
    "data[\"tweet\"] = apply_lemmatization(data[\"tweet\"])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training/test set\n",
    "\n",
    "- 80% → training set\n",
    "- 20% → test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: call the splitting of dataset for each model. This may can help the cooperatin of our agents\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"class\"], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# GPT-2\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token  # Set padding token to eos_token\n",
    "\n",
    "model_config_gpt2 = GPT2Config.from_pretrained('gpt2', num_labels=3, pad_token_id=tokenizer_gpt2.pad_token_id)\n",
    "model_gpt2 = GPT2ForSequenceClassification(model_config_gpt2).to(device)\n",
    "\n",
    "#BERT\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2\n",
    "\n",
    "The OpenAI GPT-2 (Generative Pre-Trained Transformer 2) is a transfomer-based language model. It's a standard model for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device), attention_mask\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m optimize_gpt2\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_gpt2\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     40\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/train_gpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, epoch)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1426\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1424\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1426\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1439\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1440\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:426\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    423\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m+\u001b[39m cross_attn_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m    425\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 426\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2545\u001b[0m     )\n\u001b[0;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "max_length = 128\n",
    "tokenized_texts_train = tokenizer_gpt2(list(X_train), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "tokenized_texts_test = tokenizer_gpt2(list(X_test), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Save tokenized data to disk\n",
    "torch.save(tokenized_texts_train, 'tokenized_texts_train_gpt2.pth')  \n",
    "torch.save(tokenized_texts_test, 'tokenized_texts_test_gpt2.pth')\n",
    "\n",
    "labels_train = torch.tensor(list(y_train)).to(device)\n",
    "labels_test = torch.tensor(list(y_test)).to(device)\n",
    "\n",
    "train_dataset_gpt2 = TensorDataset(tokenized_texts_train.input_ids, tokenized_texts_train.attention_mask, labels_train)\n",
    "test_dataset_gpt2 = TensorDataset(tokenized_texts_test.input_ids, tokenized_texts_test.attention_mask, labels_test)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset_gpt2, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset_gpt2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "optimize_gpt2 = AdamW(model_gpt2.parameters(), lr=5e-5, no_deprecation_warning=True)\n",
    "\n",
    "epochs = 5\n",
    "tokenized_texts_train = torch.load('tokenized_texts_train_gpt2.pth')  # Load pre-processed tokenized data from disk\n",
    "tokenized_texts_test = torch.load('tokenized_texts_test_gpt2.pth')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_gpt2.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimize_gpt2.zero_grad()\n",
    "\n",
    "        outputs = model_gpt2(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        writer.add_scalar(\"Loss/train_gpt2\", loss, epoch)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimize_gpt2.step()\n",
    "    print(f'Epoch {epoch + 1}: Average Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "torch.save(model_gpt2.state_dict(), 'gpt2_model.pth') # saving gpt-2 model\n",
    "writer.flush()\n",
    "\n",
    "model_gpt2.eval()\n",
    "predicted_labels_gpt2 = []\n",
    "true_labels_gpt2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model_gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        predicted_labels_gpt2.extend(torch.argmax(logits, axis=1).tolist()) #Extract the max logits value and convert from pytorch to a pyList\n",
    "        true_labels_gpt2.extend(labels.tolist())\n",
    "\n",
    "accuracy_gpt2 = accuracy_score(true_labels_gpt2, predicted_labels_gpt2)\n",
    "\n",
    "#saving accuracy in a file\n",
    "with open(\"accuracy_gpt2.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy_gpt2))\n",
    "    \n",
    "print(f'GPT-2 Accuracy: {accuracy_gpt2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) was developed by Google researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT model...\n",
      "Epoch 1: Average Loss: 0.39155417027011996\n",
      "Epoch 2: Average Loss: 0.2642410032210811\n",
      "Epoch 3: Average Loss: 0.22023985376281122\n",
      "Epoch 4: Average Loss: 0.167566974893693\n",
      "Epoch 5: Average Loss: 0.11658130941852446\n",
      "BERT Accuracy: 0.8741174097236232\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts_train_bert = tokenizer_bert(list(X_train), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "tokenized_texts_test_bert = tokenizer_bert(list(X_test), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Save tokenized data to disk\n",
    "torch.save(tokenized_texts_train_bert, 'tokenized_texts_train_bert.pth')  \n",
    "torch.save(tokenized_texts_test_bert, 'tokenized_texts_test_bert.pth')\n",
    "\n",
    "train_dataset_bert = TensorDataset(tokenized_texts_train_bert.input_ids, tokenized_texts_train_bert.attention_mask, labels_train)\n",
    "test_dataset_bert = TensorDataset(tokenized_texts_test_bert.input_ids, tokenized_texts_test_bert.attention_mask, labels_test)\n",
    "\n",
    "train_loader_bert = DataLoader(train_dataset_gpt2, batch_size=batch_size, shuffle=True)\n",
    "test_loader_bert = DataLoader(test_dataset_gpt2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "optimizer_bert = AdamW(model_bert.parameters(), lr=5e-5, no_deprecation_warning=True)\n",
    "tokenized_texts_train_bert = torch.load('tokenized_texts_train_bert.pth')  # Load pre-processed tokenized data from disk\n",
    "tokenized_texts_test_bert = torch.load('tokenized_texts_test_bert.pth')\n",
    "\n",
    "print(\"Training BERT model...\")\n",
    "for epoch in range(epochs):\n",
    "    model_bert.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader_bert:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer_bert.zero_grad()\n",
    "        \n",
    "        outputs = model_bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        writer.add_scalar(\"Loss/train_bert\", loss, epoch)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_bert.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader_bert)\n",
    "    print(f'Epoch {epoch + 1}: Average Loss: {avg_loss}')\n",
    "\n",
    "torch.save(model_bert.state_dict(), 'bert_model.pth') # saving bert model\n",
    "writer.flush()\n",
    "writer.close()\n",
    "# Evaluation for BERT\n",
    "model_bert.eval()\n",
    "predicted_labels_bert = []\n",
    "true_labels_bert = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_bert:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model_bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predicted_labels_bert.extend(torch.argmax(logits, axis=1).tolist())\n",
    "        true_labels_bert.extend(labels.tolist())\n",
    "\n",
    "accuracy_bert = accuracy_score(true_labels_bert, predicted_labels_bert)\n",
    "\n",
    "#saving accuracy in a file\n",
    "with open(\"accuracy_bert.txt\", \"w\") as file:\n",
    "    file.write(str(accuracy_bert))\n",
    "\n",
    "print(f'BERT Accuracy: {accuracy_bert}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on some sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: weather is wonderful today\n",
      "BERT Response: Class 2 - message approved\n",
      "GPT-2 Response: Class 2 - message approved\n",
      "\n",
      "Cooperative result: Class 2.000 - message approved\n",
      "\n",
      "Input: fucking bitch you are so stupid\n",
      "BERT Response: Class 1 - offensive language\n",
      "GPT-2 Response: Class 1 - offensive language\n",
      "\n",
      "Cooperative result: Class 1.000 - offensive language\n",
      "\n",
      "Input: go back to africa nigga\n",
      "BERT Response: Class 0 - hate speech\n",
      "GPT-2 Response: Class 0 - hate speech\n",
      "\n",
      "Cooperative result: Class 0.000 - hate speech\n",
      "\n",
      "Input: stupid autistic nigga\n",
      "BERT Response: Class 0 - hate speech\n",
      "GPT-2 Response: Class 1 - offensive language\n",
      "\n",
      "Cooperative result: Class 0.499 - hate speech\n",
      "\n",
      "Input: you are a beatiful butterfly\n",
      "BERT Response: Class 2 - message approved\n",
      "GPT-2 Response: Class 1 - offensive language\n",
      "\n",
      "Cooperative result: Class 1.501 - message approved\n",
      "\n",
      "Input: The dishes at the belzoni restourant are good\n",
      "BERT Response: Class 2 - message approved\n",
      "GPT-2 Response: Class 2 - message approved\n",
      "\n",
      "Cooperative result: Class 2.000 - message approved\n",
      "\n",
      "Input: Stupid goat\n",
      "BERT Response: Class 2 - message approved\n",
      "GPT-2 Response: Class 1 - offensive language\n",
      "\n",
      "Cooperative result: Class 1.501 - message approved\n",
      "\n",
      "Input: I'm dreaming that a bomb fall in your city\n",
      "BERT Response: Class 2 - message approved\n",
      "GPT-2 Response: Class 1 - offensive language\n",
      "\n",
      "Cooperative result: Class 1.501 - message approved\n",
      "\n",
      "Input: Son of bitch\n",
      "BERT Response: Class 1 - offensive language\n",
      "GPT-2 Response: Class 1 - offensive language\n",
      "\n",
      "Cooperative result: Class 1.000 - offensive language\n",
      "\n",
      "Input: love you son of a bitch\n",
      "BERT Response: Class 1 - offensive language\n",
      "GPT-2 Response: Class 1 - offensive language\n",
      "\n",
      "Cooperative result: Class 1.000 - offensive language\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentences = [\"weather is wonderful today\", \"fucking bitch you are so stupid\", \"go back to africa nigga\", \"stupid autistic nigga\", \"you are a beatiful butterfly\", \"The dishes at the belzoni restourant are good\", \"Stupid goat\", \"I'm dreaming that a bomb fall in your city\", \"Son of bitch\", \"love you son of a bitch\"]\n",
    "\n",
    "messages = [\"hate speech\", \"offensive language\", \"message approved\"]\n",
    "\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token  # Set padding token to eos_token\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "model_gpt2.load_state_dict(torch.load('gpt2_model.pth', map_location = device))  # Load pre-trained GPT-2 model \n",
    "model_bert.load_state_dict(torch.load('bert_model.pth', map_location = device))  # Load pre-trained BERT model from disk\n",
    "\n",
    "\n",
    "# Preprocess the input sentences\n",
    "\n",
    "input_sentences_tokenized = clean_text(input_sentences)\n",
    "input_sentences_tokenized = remove_stopwords(input_sentences_tokenized)\n",
    "input_sentences_tokenized = apply_stemming(input_sentences_tokenized)\n",
    "input_sentences_tokenized = apply_lemmatization(input_sentences_tokenized)\n",
    "\n",
    "\n",
    "# Load accuracies\n",
    "with open(\"accuracy_gpt2.txt\", \"r\") as file:\n",
    "    accuracy_gpt2 = float(file.read())\n",
    "\n",
    "with open(\"accuracy_bert.txt\", \"r\") as file:\n",
    "    accuracy_bert = float(file.read())\n",
    "\n",
    "for input_sentence in input_sentences:\n",
    "    print(f\"Input: {input_sentence}\")\n",
    "\n",
    "    # Tokenize input for BERT\n",
    "    input_ids_bert = tokenizer_bert.encode(input_sentence, return_tensors='pt').to(device)\n",
    "    attention_mask_bert = torch.ones_like(input_ids_bert).to(device)\n",
    "\n",
    "    # Generate output for BERT\n",
    "    with torch.no_grad():\n",
    "        outputs_bert = model_bert(input_ids_bert, attention_mask=attention_mask_bert)\n",
    "        predicted_class_bert = torch.argmax(outputs_bert.logits, dim=1).item()\n",
    "\n",
    "    print(f\"BERT Response: Class {predicted_class_bert} - {messages[predicted_class_bert]}\")\n",
    "\n",
    "    # Tokenize input for GPT-2\n",
    "    input_ids_gpt2 = tokenizer_gpt2.encode(input_sentence, return_tensors='pt').to(device)\n",
    "    attention_mask_gpt2 = torch.ones_like(input_ids_gpt2).to(device)\n",
    "\n",
    "    # Generate output for GPT-2\n",
    "    with torch.no_grad():\n",
    "        outputs_gpt2 = model_gpt2(input_ids_gpt2, attention_mask=attention_mask_gpt2)\n",
    "        predicted_class_gpt2 = torch.argmax(outputs_gpt2.logits, dim=1).item()\n",
    "\n",
    "    print(f\"GPT-2 Response: Class {predicted_class_gpt2} - {messages[predicted_class_gpt2]}\\n\")\n",
    "\n",
    "    # Multi-agent system: GPT-2 and BERT cooperate to guarantee a correct solution, based on their accuracy\n",
    "    predicted_class_coop = (predicted_class_bert * accuracy_bert + predicted_class_gpt2 * accuracy_gpt2) / (accuracy_gpt2 + accuracy_bert)\n",
    "    print(f\"Cooperative result: Class {predicted_class_coop:.3f} - {messages[round(predicted_class_coop)]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ['GPT-2', 'BERT']\n",
    "accuracies = [accuracy_gpt2, accuracy_bert]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(models, accuracies, color=['blue', 'green'])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparison of Accuracy Between GPT-2 and BERT')\n",
    "plt.ylim(0, 1)  # Set the y-axis limit from 0 to 1\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
